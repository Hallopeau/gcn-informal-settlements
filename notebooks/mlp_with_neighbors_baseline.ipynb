{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a8316-4aec-42c5-aed8-f0756096f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    cohen_kappa_score,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Load and preprocess geospatial data\n",
    "# =============================================================================\n",
    "mesh = gpd.read_file(\"/home/stagiaire/Téléchargements/PR/D/mesh_rj.shp\")\n",
    "mesh[\"label\"] = np.where(\n",
    "    (mesh[\"vegetation\"] <= 0.95)\n",
    "    & (mesh[\"ghsl\"] >= 0.5)\n",
    "    & (mesh[\"osm\"] <= 0.1)\n",
    "    & (mesh[\"favelas\"] > 0.9),\n",
    "    1,\n",
    "    np.where(\n",
    "        (mesh[\"vegetation\"] <= 0.95)\n",
    "        & (mesh[\"ghsl\"] >= 0.5)\n",
    "        & (mesh[\"osm\"] <= 0.1)\n",
    "        & (mesh[\"favelas\"] == 0),\n",
    "        0,\n",
    "        np.nan,\n",
    "    ),\n",
    ")\n",
    "dataset = mesh[mesh[\"label\"].notna()].copy()\n",
    "\n",
    "zones = gpd.read_file(\"/home/stagiaire/Téléchargements/PR/D/zones.shp\")\n",
    "dataset[\"centroid\"] = dataset.geometry.centroid\n",
    "points_zones = gpd.sjoin(\n",
    "    dataset.set_geometry(\"centroid\"),\n",
    "    zones[[\"fid\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"within\",\n",
    ")\n",
    "dataset[\"zone\"] = points_zones[\"fid\"]\n",
    "dataset.drop(columns=[\"centroid\"], inplace=True)\n",
    "dataset = dataset[dataset[\"zone\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Build feature matrix including up to 8 neighbors\n",
    "# =============================================================================\n",
    "# Identify all pairs of intersecting cells (including corner-touching)\n",
    "neighbors_df = gpd.sjoin(\n",
    "    dataset, dataset, how=\"inner\", predicate=\"intersects\",\n",
    "    lsuffix=\"left\", rsuffix=\"right\"\n",
    ")\n",
    "neighbors_df = neighbors_df[neighbors_df.index != neighbors_df[\"index_right\"]]\n",
    "\n",
    "feature_cols = [\n",
    "    \"vegetation\", \"slope\", \"profile_co\", \"entropy\",\n",
    "    \"nodes\", \"roads\", \"mean_conne\", \"min_connex\", \"max_connex\",\n",
    "]\n",
    "num_features = len(feature_cols)\n",
    "max_neighbors = 8\n",
    "\n",
    "X_list, y_list, groups_list = [], [], []\n",
    "cell_features = {\n",
    "    int(r[\"id\"]): r[feature_cols].values.astype(np.float32)\n",
    "    for _, r in dataset.iterrows()\n",
    "}\n",
    "\n",
    "for _, row in dataset.iterrows():\n",
    "    cell_id = int(row[\"id\"])\n",
    "    groups_list.append(int(row[\"zone\"]))\n",
    "\n",
    "    # Start with the 9 features of the target cell\n",
    "    feat_vec = list(cell_features[cell_id])\n",
    "\n",
    "    # Add up to 8 neighbors’ features\n",
    "    nbrs = neighbors_df[\n",
    "        neighbors_df[\"id_left\"] == cell_id\n",
    "    ].head(max_neighbors)\n",
    "    for _, nbr in nbrs.iterrows():\n",
    "        feat_vec.extend(cell_features[int(nbr[\"id_right\"])])\n",
    "\n",
    "    # Zero-pad if fewer than 8 neighbors\n",
    "    while len(feat_vec) < (1 + max_neighbors) * num_features:\n",
    "        feat_vec.extend([0.0] * num_features)\n",
    "\n",
    "    X_list.append(feat_vec)\n",
    "    y_list.append(int(row[\"label\"]))\n",
    "\n",
    "X = torch.tensor(np.array(X_list, dtype=np.float32))\n",
    "y = torch.tensor(np.array(y_list, dtype=np.int64))\n",
    "groups = np.array(groups_list)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Number of examples:\", X.shape[0])\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Undersampling function\n",
    "# =============================================================================\n",
    "def undersample(idx, labels):\n",
    "    \"\"\"Random undersample to balance binary classes within idx.\"\"\"\n",
    "    idx = np.array(idx)\n",
    "    labs = labels[idx].numpy()\n",
    "    c0, c1 = idx[labs == 0], idx[labs == 1]\n",
    "    if len(c0) == 0 or len(c1) == 0:\n",
    "        return idx\n",
    "    m = min(len(c0), len(c1))\n",
    "    res = np.concatenate([\n",
    "        np.random.choice(c0, m, replace=False),\n",
    "        np.random.choice(c1, m, replace=False),\n",
    "    ])\n",
    "    np.random.shuffle(res)\n",
    "    return res\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Define MLP model\n",
    "# =============================================================================\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Single-hidden-layer MLP for binary classification.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Train and evaluate function\n",
    "# =============================================================================\n",
    "def train_and_evaluate_with_neighbors(train_idx, test_idx):\n",
    "    \"\"\"Train and evaluate the MLP using neighbor-augmented features.\"\"\"\n",
    "    train_idx_bal = undersample(train_idx, y)\n",
    "    test_idx_bal = undersample(test_idx, y)\n",
    "\n",
    "    X_train, y_train = X[train_idx_bal], y[train_idx_bal]\n",
    "    X_test, y_test = X[test_idx_bal], y[test_idx_bal]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train), batch_size=32, shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        TensorDataset(X_test, y_test), batch_size=32, shuffle=False\n",
    "    )\n",
    "\n",
    "    model = MLP(\n",
    "        input_dim=X.shape[1],\n",
    "        hidden_dim=64,\n",
    "        output_dim=2,\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, 401):\n",
    "        for bx, by in train_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(bx), by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch:03d} complete\")\n",
    "\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for bx, by in test_loader:\n",
    "            out = model(bx.to(device)).argmax(dim=1).cpu()\n",
    "            preds.append(out)\n",
    "            trues.append(by)\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    trues = torch.cat(trues).numpy()\n",
    "\n",
    "    p = precision_score(trues, preds, zero_division=0)\n",
    "    r = recall_score(trues, preds, zero_division=0)\n",
    "    f1 = f1_score(trues, preds, zero_division=0)\n",
    "    k = cohen_kappa_score(trues, preds)\n",
    "    print(f\"P:{p:.3f} R:{r:.3f} F1:{f1:.3f} K:{k:.3f}\")\n",
    "    return p, r, f1, k\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Serial spatial cross-validation (10 × 5 folds)\n",
    "# =============================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_iters = 10\n",
    "n_folds = 5\n",
    "metrics_iters = np.zeros((n_iters, n_folds, 4))  # P, R, F1, Kappa\n",
    "\n",
    "for it in range(n_iters):\n",
    "    print(f\"\\n=== Iteration {it + 1}/{n_iters} ===\")\n",
    "    gkf = GroupKFold(n_splits=n_folds)\n",
    "    for fold, (tr_idx, te_idx) in enumerate(\n",
    "        gkf.split(X.numpy(), y.numpy(), groups),\n",
    "        start=1\n",
    "    ):\n",
    "        print(f\"\\n-- Fold {fold} --\")\n",
    "        metrics_iters[it, fold - 1] = train_and_evaluate_with_neighbors(tr_idx, te_idx)\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Aggregate and print results\n",
    "# =============================================================================\n",
    "mean_per_fold = metrics_iters.mean(axis=0)\n",
    "std_per_fold = metrics_iters.std(axis=0)\n",
    "global_mean = mean_per_fold.mean(axis=0)\n",
    "global_std = mean_per_fold.std(axis=0)\n",
    "\n",
    "print(\"\\n--- Per-fold averages ---\")\n",
    "for f in range(n_folds):\n",
    "    print(\n",
    "        f\"Fold {f+1}: \"\n",
    "        f\"P {mean_per_fold[f,0]:.3f}±{std_per_fold[f,0]:.3f}, \"\n",
    "        f\"R {mean_per_fold[f,1]:.3f}±{std_per_fold[f,1]:.3f}, \"\n",
    "        f\"F1 {mean_per_fold[f,2]:.3f}±{std_per_fold[f,2]:.3f}, \"\n",
    "        f\"K {mean_per_fold[f,3]:.3f}±{std_per_fold[f,3]:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    \"\\n--- Global averages ---\\n\"\n",
    "    f\"P {global_mean[0]:.3f}±{global_std[0]:.3f}, \"\n",
    "    f\"R {global_mean[1]:.3f}±{global_std[1]:.3f}, \"\n",
    "    f\"F1 {global_mean[2]:.3f}±{global_std[2]:.3f}, \"\n",
    "    f\"K {global_mean[3]:.3f}±{global_std[3]:.3f}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
