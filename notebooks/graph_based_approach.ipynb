{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925ccc0-a5ef-402b-a26f-c97408f14cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx, k_hop_subgraph\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    cohen_kappa_score,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Load and preprocess geospatial data\n",
    "# =============================================================================\n",
    "mesh = gpd.read_file(\"/home/stagiaire/Téléchargements/PR/D/mesh_rj.shp\")\n",
    "mesh[\"label\"] = np.where(\n",
    "    (mesh[\"vegetation\"] <= 0.95)\n",
    "    & (mesh[\"ghsl\"] >= 0.5)\n",
    "    & (mesh[\"osm\"] <= 0.1)\n",
    "    & (mesh[\"favelas\"] > 0.9),\n",
    "    1,\n",
    "    np.where(\n",
    "        (mesh[\"vegetation\"] <= 0.95)\n",
    "        & (mesh[\"ghsl\"] >= 0.5)\n",
    "        & (mesh[\"osm\"] <= 0.1)\n",
    "        & (mesh[\"favelas\"] == 0),\n",
    "        0,\n",
    "        np.nan,\n",
    "    ),\n",
    ")\n",
    "dataset = mesh[mesh[\"label\"].notna()].copy()\n",
    "\n",
    "zones = gpd.read_file(\"/home/stagiaire/Téléchargements/PR/D/zones.shp\")\n",
    "dataset[\"centroid\"] = dataset.geometry.centroid\n",
    "points_zones = gpd.sjoin(\n",
    "    dataset.set_geometry(\"centroid\"), zones[[\"fid\", \"geometry\"]],\n",
    "    how=\"left\", predicate=\"within\"\n",
    ")\n",
    "dataset[\"zone\"] = points_zones[\"fid\"]\n",
    "dataset.drop(columns=[\"centroid\"], inplace=True)\n",
    "dataset = dataset[dataset[\"zone\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Build planar adjacency graph\n",
    "# =============================================================================\n",
    "feature_cols = [\n",
    "    \"vegetation\", \"slope\", \"profile_co\", \"entropy\",\n",
    "    \"nodes\", \"roads\", \"mean_conne\", \"min_connex\", \"max_connex\",\n",
    "]\n",
    "G = nx.Graph()\n",
    "for _, row in dataset.iterrows():\n",
    "    feats = torch.tensor(row[feature_cols].values, dtype=torch.float32)\n",
    "    G.add_node(int(row[\"id\"]), x=feats, label=int(row[\"label\"]))\n",
    "for _, row in dataset.iterrows():\n",
    "    neigh = dataset[dataset.geometry.touches(row.geometry)]\n",
    "    for _, nr in neigh.iterrows():\n",
    "        G.add_edge(int(row[\"id\"]), int(nr[\"id\"]))\n",
    "\n",
    "pyg_data = from_networkx(G, group_node_attrs=[\"x\"])\n",
    "pyg_data.y = torch.tensor(\n",
    "    [G.nodes[n][\"label\"] for n in G.nodes()], dtype=torch.long\n",
    ")\n",
    "zone_map = {int(r[\"id\"]): int(r[\"zone\"]) for _, r in dataset.iterrows()}\n",
    "node_ids = list(G.nodes())\n",
    "groups = np.array([zone_map[n] for n in node_ids])\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Ego‑graph extraction\n",
    "# =============================================================================\n",
    "def extract_ego_graph(data, center, hops=1):\n",
    "    subset, edge_idx, mapping, _ = k_hop_subgraph(\n",
    "        center, hops, data.edge_index, relabel_nodes=True\n",
    "    )\n",
    "    mapping = int(mapping)\n",
    "    if mapping != 0:\n",
    "        order = [mapping] + [i for i in range(len(subset)) if i != mapping]\n",
    "        perm = torch.tensor(order, dtype=torch.long)\n",
    "        sub_x = data.x[subset][perm]\n",
    "        inv = {old: new for new, old in enumerate(perm.tolist())}\n",
    "        new_ei = edge_idx.clone()\n",
    "        for i in range(new_ei.size(1)):\n",
    "            new_ei[0, i] = inv[int(new_ei[0, i])]\n",
    "            new_ei[1, i] = inv[int(new_ei[1, i])]\n",
    "        center_idx = 0\n",
    "    else:\n",
    "        sub_x = data.x[subset]\n",
    "        new_ei = edge_idx\n",
    "        center_idx = 0\n",
    "\n",
    "    g = Data(x=sub_x, edge_index=new_ei)\n",
    "    g.y = data.y[center].unsqueeze(0)\n",
    "    g.center_idx = center_idx\n",
    "    return g\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Undersampling to balance classes\n",
    "# =============================================================================\n",
    "def undersample(indices, labels):\n",
    "    idx = np.array(indices)\n",
    "    labs = labels[idx].numpy()\n",
    "    c0, c1 = idx[labs == 0], idx[labs == 1]\n",
    "    if len(c0) == 0 or len(c1) == 0:\n",
    "        return idx\n",
    "    m = min(len(c0), len(c1))\n",
    "    res = np.concatenate([\n",
    "        np.random.choice(c0, m, replace=False),\n",
    "        np.random.choice(c1, m, replace=False),\n",
    "    ])\n",
    "    np.random.shuffle(res)\n",
    "    return res\n",
    "\n",
    "# =============================================================================\n",
    "# 5. GCN model definition\n",
    "# =============================================================================\n",
    "class EgoGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim)\n",
    "        self.conv2 = GCNConv(hid_dim, hid_dim)\n",
    "        self.classifier = nn.Linear(hid_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_idx = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_idx))\n",
    "        x = F.relu(self.conv2(x, edge_idx))\n",
    "        center = data.ptr[0].item()\n",
    "        out = self.classifier(x[center : center + 1])\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "def train_epoch(loader, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(batch), batch.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * batch.num_graphs\n",
    "    return loss_sum / len(loader.dataset)\n",
    "\n",
    "def test_epoch(loader, model, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch).argmax(dim=1)\n",
    "        correct += (pred == batch.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Serial spatial cross‑validation (10×5 folds)\n",
    "# =============================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "in_dim = pyg_data.x.size(1)\n",
    "hid_dim = 64\n",
    "num_classes = len(torch.unique(pyg_data.y))\n",
    "\n",
    "n_iters = 10\n",
    "n_folds = 5\n",
    "metrics_iters = np.zeros((n_iters, n_folds, 4))  # P, R, F1, Kappa\n",
    "\n",
    "for it in range(n_iters):\n",
    "    print(f\"\\n=== Iteration {it + 1}/{n_iters} ===\")\n",
    "    gkf = GroupKFold(n_splits=n_folds)\n",
    "    for fold, (train_idx, test_idx) in enumerate(\n",
    "        gkf.split(np.arange(pyg_data.num_nodes), pyg_data.y.numpy(), groups),\n",
    "        start=1,\n",
    "    ):\n",
    "        # balance and extract ego-graphs\n",
    "        t_idx = undersample(train_idx, pyg_data.y)\n",
    "        v_idx = undersample(test_idx, pyg_data.y)\n",
    "        train_graphs = [extract_ego_graph(pyg_data, int(i)) for i in t_idx]\n",
    "        test_graphs = [extract_ego_graph(pyg_data, int(i)) for i in v_idx]\n",
    "\n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_graphs, batch_size=32)\n",
    "\n",
    "        model = EgoGCN(in_dim, hid_dim, num_classes).to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "\n",
    "        # train\n",
    "        for epoch in range(1, 401):\n",
    "            loss = train_epoch(train_loader, model, opt, crit, device)\n",
    "            if epoch % 100 == 0:\n",
    "                acc = test_epoch(test_loader, model, device)\n",
    "                print(f\"Fold {fold} | Epoch {epoch:03d} | Loss: {loss:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "        # evaluate\n",
    "        all_preds, all_trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                p = model(batch).argmax(dim=1).cpu()\n",
    "                all_preds.append(p)\n",
    "                all_trues.append(batch.y.cpu())\n",
    "        preds = torch.cat(all_preds).numpy()\n",
    "        trues = torch.cat(all_trues).numpy()\n",
    "\n",
    "        p = precision_score(trues, preds)\n",
    "        r = recall_score(trues, preds, zero_division=0)\n",
    "        f1 = f1_score(trues, preds, zero_division=0)\n",
    "        k = cohen_kappa_score(trues, preds)\n",
    "\n",
    "        print(f\"Fold {fold} | P:{p:.3f} R:{r:.3f} F1:{f1:.3f} K:{k:.3f}\")\n",
    "        metrics_iters[it, fold - 1] = [p, r, f1, k]\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Aggregate serial CV results\n",
    "# =============================================================================\n",
    "mean_fold = metrics_iters.mean(axis=0)\n",
    "std_fold = metrics_iters.std(axis=0)\n",
    "\n",
    "print(\"\\n--- Per-fold averages over iterations ---\")\n",
    "for fold in range(n_folds):\n",
    "    print(\n",
    "        f\"Fold {fold+1}: \"\n",
    "        f\"P {mean_fold[fold,0]:.3f}±{std_fold[fold,0]:.3f} \"\n",
    "        f\"R {mean_fold[fold,1]:.3f}±{std_fold[fold,1]:.3f} \"\n",
    "        f\"F1 {mean_fold[fold,2]:.3f}±{std_fold[fold,2]:.3f} \"\n",
    "        f\"K {mean_fold[fold,3]:.3f}±{std_fold[fold,3]:.3f}\"\n",
    "    )\n",
    "\n",
    "global_mean = mean_fold.mean(axis=0)\n",
    "global_std = mean_fold.std(axis=0)\n",
    "print(\n",
    "    \"\\n--- Global averages ---\\n\"\n",
    "    f\"P {global_mean[0]:.3f}±{global_std[0]:.3f} \"\n",
    "    f\"R {global_mean[1]:.3f}±{global_std[1]:.3f} \"\n",
    "    f\"F1 {global_mean[2]:.3f}±{global_std[2]:.3f} \"\n",
    "    f\"K {global_mean[3]:.3f}±{global_std[3]:.3f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
